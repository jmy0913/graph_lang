{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# 회계 문서 검색 시스템 성능 평가 보고서\n",
    "\n",
    "## 1. 개요\n",
    "\n",
    "본 보고서는 회계 문서 검색 시스템에서 **BM25 알고리즘**과 **기본 검색 방법**의 성능을 비교 평가한 결과를 제시합니다.\n",
    "\n",
    "### 1.1 평가 목적\n",
    "- 회계 관련 문서 검색에서 BM25 알고리즘의 효과성 검증\n",
    "- 기존 검색 방법 대비 성능 개선 정도 측정\n",
    "- 향후 시스템 개선 방향 제시\n",
    "\n",
    "### 1.2 평가 지표\n",
    "- **Precision@5**: 상위 5개 결과 중 관련 문서 비율\n",
    "- **Recall@5**: 전체 관련 문서 중 상위 5개에서 검색된 비율\n",
    "- **MRR (Mean Reciprocal Rank)**: 첫 번째 관련 문서의 평균 역순위\n",
    "- **MAP (Mean Average Precision)**: 평균 정밀도\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 실험 설정\n",
    "\n",
    "### 2.1 라이브러리 import 및 초기 설정\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils1.retreiver_setting import faiss_retriever_loading\n",
    "from utils1.chain_setting import create_chain\n",
    "from transformers import BertTokenizer\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# 검색기 및 체인 로딩\n",
    "account_retriever, business_retriever, business_retriever2, self_retriever = faiss_retriever_loading()\n",
    "simple_chain, classification_chain, account_chain, extract_chain, business_chain, hybrid_chain, financial_chain = create_chain()\n",
    "```\n",
    "\n",
    "### 2.2 BM25 전처리 함수\n",
    "\n",
    "```python\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    BERT 토크나이저를 사용한 한국어 텍스트 전처리\n",
    "    \"\"\"\n",
    "    tokenizer = BertTokenizer.from_pretrained('kykim/bert-kor-base')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def calculate_bm25(query, documents):\n",
    "    \"\"\"\n",
    "    BM25 점수 계산 함수\n",
    "    \"\"\"\n",
    "    # 문서 토큰화 (BERT tokenizer 사용)\n",
    "    corpus = [preprocess(doc.page_content) for doc in documents]\n",
    "\n",
    "    # BM25 모델 적용\n",
    "    bm25 = BM25Okapi(corpus)\n",
    "\n",
    "    # 쿼리 토큰화\n",
    "    query_tokens = preprocess(query)\n",
    "    scores = bm25.get_scores(query_tokens)\n",
    "\n",
    "    return scores\n",
    "```\n",
    "\n",
    "### 2.3 검색 함수 구현\n",
    "\n",
    "```python\n",
    "def bm25_search(query, top_k=5):\n",
    "    \"\"\"\n",
    "    BM25 기반 검색 함수\n",
    "    \"\"\"\n",
    "    documents = account_retriever.invoke(query)\n",
    "    bm25_scores = calculate_bm25(query, documents)\n",
    "\n",
    "    # 점수 기준 정렬 후 상위 k개 반환\n",
    "    sorted_docs = sorted(zip(documents, bm25_scores), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    return [{'id': doc.id, 'content': doc.page_content} for doc, _ in sorted_docs]\n",
    "\n",
    "def handle_accounting_non_bm25(question: str) -> list:\n",
    "    \"\"\"\n",
    "    기본 검색 방법 (BM25 미적용)\n",
    "    \"\"\"\n",
    "    docs = account_retriever.invoke(question)\n",
    "    docs = docs[:5]  # 상위 5개 문서\n",
    "    return [{'id': doc.id, 'content': doc.page_content} for doc in docs]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 평가 메트릭 구현\n",
    "\n",
    "### 3.1 성능 지표 계산 함수\n",
    "\n",
    "```python\n",
    "def compute_metrics(predicted, relevant_dict, k=5):\n",
    "    \"\"\"\n",
    "    Precision@k, Recall@k, MRR, AP 계산\n",
    "    \"\"\"\n",
    "    # Precision@k: 상위 k 중 관련 문서 비율\n",
    "    hits = sum([1 for doc in predicted[:k] if doc['id'] in relevant_dict])\n",
    "    precision = hits / k\n",
    "\n",
    "    # Recall@k: 관련 문서 총 개수 대비 상위 k 중 회수된 관련 개수\n",
    "    total_relevant = len(relevant_dict)\n",
    "    recall = hits / total_relevant if total_relevant > 0 else 0\n",
    "\n",
    "    # MRR: 첫 번째 관련 문서 위치 기반\n",
    "    rr = 0\n",
    "    for idx, doc in enumerate(predicted):\n",
    "        if doc['id'] in relevant_dict:\n",
    "            rr = 1 / (idx + 1)\n",
    "            break\n",
    "\n",
    "    # Average Precision 계산\n",
    "    num_correct = 0\n",
    "    precisions = []\n",
    "    for i, doc in enumerate(predicted[:k]):\n",
    "        if doc['id'] in relevant_dict:\n",
    "            num_correct += 1\n",
    "            precisions.append(num_correct / (i + 1))\n",
    "    ap = np.mean(precisions) if precisions else 0\n",
    "\n",
    "    return precision, recall, rr, ap\n",
    "\n",
    "def evaluate_all(method_results, queries, k=5):\n",
    "    \"\"\"\n",
    "    모든 쿼리에 대해 성능 평가 수행\n",
    "    \"\"\"\n",
    "    prec_list, rec_list, rr_list, ap_list = [], [], [], []\n",
    "\n",
    "    for query in queries:\n",
    "        qid = query['query_id']\n",
    "        relevant_dict = parse_relevant(query['relevant_doc_ids'])\n",
    "        predicted = method_results[qid]\n",
    "\n",
    "        p, r, rr, ap = compute_metrics(predicted, relevant_dict, k)\n",
    "\n",
    "        prec_list.append(p)\n",
    "        rec_list.append(r)\n",
    "        rr_list.append(rr)\n",
    "        ap_list.append(ap)\n",
    "\n",
    "    return {\n",
    "        'P@5': np.mean(prec_list),\n",
    "        'R@5': np.mean(rec_list),\n",
    "        'MRR': np.mean(rr_list),\n",
    "        'MAP': np.mean(ap_list)\n",
    "    }\n",
    "\n",
    "def parse_relevant(relevant_doc_ids):\n",
    "    \"\"\"\n",
    "    관련 문서 ID 파싱\n",
    "    \"\"\"\n",
    "    return {doc_id.split('=')[0] for doc_id in relevant_doc_ids.split(';')}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 테스트 데이터셋\n",
    "\n",
    "### 4.1 테스트 쿼리 정의\n",
    "\n",
    "```python\n",
    "queries = [\n",
    "    {\n",
    "        \"query_id\": \"Q1\",\n",
    "        \"query_text\": \"기업회계기준서 제1109호 금융상품 관련\",\n",
    "        \"relevant_doc_ids\": \"a9792da2-2636-400e-a37b-6d7ce7547778=1\"\n",
    "    },\n",
    "    {\n",
    "        \"query_id\": \"Q2\",\n",
    "        \"query_text\": \"사업결합 관련 기업회계기준서\",\n",
    "        \"relevant_doc_ids\": \"27da9efc-1aa5-4ab3-98f0-3a0e10ba2b9c=1;256e05b0-43a5-43d4-b696-7c7405abc463=2\"\n",
    "    },\n",
    "    {\n",
    "        \"query_id\": \"Q3\",\n",
    "        \"query_text\": \"회계정책과 회계추정치 변경 관련\",\n",
    "        \"relevant_doc_ids\": \"19df546f-a4ce-4b40-8971-2730cc6e24f4=1;256e05b0-43a5-43d4-b696-7c7405abc463=2\"\n",
    "    },\n",
    "    {\n",
    "        \"query_id\": \"Q4\",\n",
    "        \"query_text\": \"농림어업 관련 회계기준서\",\n",
    "        \"relevant_doc_ids\": \"f15bf88e-1f13-44d2-88fb-0ea1f67633cc=1;5e47c132-1b65-4476-bdac-6fc5b4089fea=2\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"테스트 쿼리 총 개수:\", len(queries))\n",
    "for query in queries:\n",
    "    print(f\"- {query['query_id']}: {query['query_text']}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. 실험 실행\n",
    "\n",
    "### 5.1 BM25 방법 평가\n",
    "\n",
    "```python\n",
    "print(\"BM25 검색 방법 평가 중...\")\n",
    "bm25_results = {}\n",
    "\n",
    "for query in queries:\n",
    "    qid = query['query_id']\n",
    "    query_text = query['query_text']\n",
    "    bm25_results[qid] = bm25_search(query_text, top_k=5)\n",
    "    print(f\"✓ {qid} 완료\")\n",
    "\n",
    "bm25_evaluation = evaluate_all(bm25_results, queries, k=5)\n",
    "print(\"BM25 평가 완료\")\n",
    "```\n",
    "\n",
    "### 5.2 기본 검색 방법 평가\n",
    "\n",
    "```python\n",
    "print(\"\\n기본 검색 방법 평가 중...\")\n",
    "non_bm25_results = {}\n",
    "\n",
    "for query in queries:\n",
    "    qid = query['query_id']\n",
    "    query_text = query['query_text']\n",
    "    non_bm25_results[qid] = handle_accounting_non_bm25(query_text)\n",
    "    print(f\"✓ {qid} 완료\")\n",
    "\n",
    "non_bm25_evaluation = evaluate_all(non_bm25_results, queries, k=5)\n",
    "print(\"기본 검색 방법 평가 완료\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. 결과 분석\n",
    "\n",
    "### 6.1 성능 지표 비교\n",
    "\n",
    "```python\n",
    "print(\"=== 성능 평가 결과 ===\")\n",
    "print(\"\\nBM25 방법:\")\n",
    "for metric, value in bm25_evaluation.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\n기본 검색 방법:\")\n",
    "for metric, value in non_bm25_evaluation.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "```\n",
    "\n",
    "### 6.2 결과 시각화\n",
    "\n",
    "```python\n",
    "# 결과를 DataFrame으로 정리\n",
    "results_df = pd.DataFrame([bm25_evaluation, non_bm25_evaluation],\n",
    "                         index=['BM25', 'Non-BM25'])\n",
    "\n",
    "print(\"\\n=== 성능 비교 표 ===\")\n",
    "print(results_df.round(4))\n",
    "\n",
    "\n",
    "## 부록: 상세 실험 로그\n",
    "\n",
    "```python\n",
    "# 상세 결과 출력을 위한 함수\n",
    "def print_detailed_results(method_name, results, queries):\n",
    "    print(f\"\\n=== {method_name} 상세 결과 ===\")\n",
    "    for query in queries:\n",
    "        qid = query['query_id']\n",
    "        query_text = query['query_text']\n",
    "        relevant_dict = parse_relevant(query['relevant_doc_ids'])\n",
    "        predicted = results[qid]\n",
    "\n",
    "        p, r, rr, ap = compute_metrics(predicted, relevant_dict, k=5)\n",
    "\n",
    "        print(f\"\\n{qid}: {query_text}\")\n",
    "        print(f\"  관련 문서 수: {len(relevant_dict)}\")\n",
    "        print(f\"  P@5: {p:.4f}, R@5: {r:.4f}, MRR: {rr:.4f}, AP: {ap:.4f}\")\n",
    "\n",
    "# 상세 결과 출력\n",
    "print_detailed_results(\"BM25\", bm25_results, queries)\n",
    "print_detailed_results(\"기본 검색\", non_bm25_results, queries)\n",
    "```"
   ],
   "id": "f9642030cd1d36d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 실제 코드 실행 내용 (보고서 코드와 변수명은 조금 다를 수 있습니다.)",
   "id": "548b302ffa70264d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T09:22:02.352237Z",
     "start_time": "2025-07-16T09:21:53.922389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from JeongMinYoung.utils1.retreiver_setting import faiss_retriever_loading\n",
    "from JeongMinYoung.utils1.chain_setting import create_chain\n",
    "from transformers import BertTokenizer\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "\n",
    "\n",
    "account_retriever, business_retriever, business_retriever2, self_retriever = faiss_retriever_loading()\n",
    "simple_chain, classification_chain, account_chain, extract_chain, business_chain, hybrid_chain, financial_chain = create_chain()"
   ],
   "id": "d674464fd6d4fc5a",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T09:22:02.365377Z",
     "start_time": "2025-07-16T09:22:02.357689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertTokenizer\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# 한국어 형태소 분석기\n",
    "def preprocess(text):\n",
    "    tokenizer = BertTokenizer.from_pretrained('kykim/bert-kor-base')\n",
    "    tokens = tokenizer.tokenize(text)  # BERT tokenizer로 단어 분리\n",
    "    return tokens\n",
    "\n",
    "# BM25 계산 함수\n",
    "def calculate_bm25(query, documents):\n",
    "    # 문서 토큰화 (BERT tokenizer 사용)\n",
    "    corpus = [preprocess(doc.page_content) for doc in documents]\n",
    "\n",
    "    # BM25 모델 적용\n",
    "    bm25 = BM25Okapi(corpus)\n",
    "\n",
    "    # 쿼리 토큰화 (BERT tokenizer 사용)\n",
    "    query_tokens = preprocess(query)  # 쿼리도 형태소 분석\n",
    "    scores = bm25.get_scores(query_tokens)\n",
    "\n",
    "    return scores\n",
    "\n",
    "# BM25 검색 함수\n",
    "def bm25_search(query, top_k=5):\n",
    "    documents = account_retriever.invoke(query)  # 문서 검색\n",
    "    bm25_scores = calculate_bm25(query, documents)  # BM25 점수 계산\n",
    "\n",
    "    # 문서 ID와 내용만 포함한 결과 반환\n",
    "    sorted_docs = sorted(zip(documents, bm25_scores), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    return [{'id': doc.id, 'content': doc.page_content} for doc, _ in sorted_docs]  # 문서 객체가 아닌, 문서 ID와 내용을 반환"
   ],
   "id": "94af5b30a06595c3",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "158e2a7929981c1b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T09:22:05.927076Z",
     "start_time": "2025-07-16T09:22:05.917246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_metrics(predicted, relevant_dict, k=5):\n",
    "    \"\"\"\n",
    "    Precision@k, Recall@k, MRR, AP 계산\n",
    "    \"\"\"\n",
    "    # Precision@k: 상위 k 중 관련(grade>=1) 문서 비율\n",
    "    hits = sum([1 for doc in predicted[:k] if doc['id'] in relevant_dict])  # 'id'로 문서 ID 비교\n",
    "    precision = hits / k\n",
    "\n",
    "    # Recall@k: 관련 문서 총 개수 대비 상위 k 중 회수된 관련 개수\n",
    "    total_relevant = len(relevant_dict)\n",
    "    recall = hits / total_relevant if total_relevant > 0 else 0\n",
    "\n",
    "    # MRR: 첫 번째 관련 문서 위치 기반\n",
    "    rr = 0\n",
    "    for idx, doc in enumerate(predicted):\n",
    "        if doc['id'] in relevant_dict:  # 'id'로 문서 ID 비교\n",
    "            rr = 1 / (idx + 1)\n",
    "            break\n",
    "\n",
    "    # 단일 AP 계산 (MAP를 위해)\n",
    "    num_correct = 0\n",
    "    precisions = []\n",
    "    for i, doc in enumerate(predicted[:k]):\n",
    "        if doc['id'] in relevant_dict:  # 'id'로 문서 ID 비교\n",
    "            num_correct += 1\n",
    "            precisions.append(num_correct / (i + 1))\n",
    "    ap = np.mean(precisions) if precisions else 0\n",
    "\n",
    "    return precision, recall, rr, ap\n",
    "\n",
    "\n",
    "def evaluate_all(method_results, queries, k=5):\n",
    "    \"\"\"\n",
    "    모든 쿼리에 대해 성능 평가를 수행하고 평균을 계산합니다.\n",
    "    \"\"\"\n",
    "    prec_list, rec_list, rr_list, ap_list = [], [], [], []\n",
    "\n",
    "    for query in queries:\n",
    "        qid = query['query_id']\n",
    "        relevant_dict = parse_relevant(query['relevant_doc_ids'])\n",
    "        predicted = method_results[qid]\n",
    "\n",
    "        p, r, rr, ap = compute_metrics(predicted, relevant_dict, k)\n",
    "\n",
    "        prec_list.append(p)\n",
    "        rec_list.append(r)\n",
    "        rr_list.append(rr)\n",
    "        ap_list.append(ap)\n",
    "\n",
    "    # 평균 지표 반환\n",
    "    return {\n",
    "        'P@k': np.mean(prec_list),\n",
    "        'R@k': np.mean(rec_list),\n",
    "        'MRR': np.mean(rr_list),\n",
    "        'MAP': np.mean(ap_list)\n",
    "    }"
   ],
   "id": "309913f0ba5f7a5c",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T09:22:09.158802Z",
     "start_time": "2025-07-16T09:22:06.061510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 예시로 주어진 queries 리스트\n",
    "queries = [\n",
    "    {\"query_id\": \"Q1\", \"query_text\": \"기업회계기준서 제1109호 금융상품 관련\", \"relevant_doc_ids\": \"a9792da2-2636-400e-a37b-6d7ce7547778=1\"},\n",
    "    {\"query_id\": \"Q2\", \"query_text\": \"사업결합 관련 기업회계기준서\", \"relevant_doc_ids\": \"27da9efc-1aa5-4ab3-98f0-3a0e10ba2b9c=1;256e05b0-43a5-43d4-b696-7c7405abc463=2\"},\n",
    "    {\"query_id\": \"Q3\", \"query_text\": \"회계정책과 회계추정치 변경 관련\", \"relevant_doc_ids\": \"19df546f-a4ce-4b40-8971-2730cc6e24f4=1;256e05b0-43a5-43d4-b696-7c7405abc463=2\"},\n",
    "    {\"query_id\": \"Q4\", \"query_text\": \"농림어업 관련 회계기준서\", \"relevant_doc_ids\": \"f15bf88e-1f13-44d2-88fb-0ea1f67633cc=1;5e47c132-1b65-4476-bdac-6fc5b4089fea=2\"}\n",
    "]\n",
    "\n",
    "\n",
    "def handle_accounting_non_bm25(question: str) -> list:\n",
    "    \"\"\"\n",
    "    BM25를 사용하지 않고 회계 질문에 답변하는 함수\n",
    "    \"\"\"\n",
    "    # 문서 가져오기 (BM25 미적용, 상위 5개 문서만 가져옴)\n",
    "    docs = account_retriever.invoke(question)\n",
    "    docs = docs[:5]  # BM25 적용하지 않으면 그냥 상위 5개 문서\n",
    "    return [{'id': doc.id, 'content': doc.page_content} for doc in docs]\n",
    "\n",
    "def parse_relevant(relevant_doc_ids):\n",
    "    return {doc_id.split('=')[0] for doc_id in relevant_doc_ids.split(';')}\n",
    "\n",
    "# BM25 계산 함수 예시\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "def calculate_bm25(query, documents):\n",
    "    corpus = [doc.page_content.split() for doc in documents]\n",
    "    bm25 = BM25Okapi(corpus)\n",
    "    query_tokens = query.split()  # query를 토큰화\n",
    "    scores = bm25.get_scores(query_tokens)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# 평가\n",
    "bm25_results = {}\n",
    "for query in queries:\n",
    "    qid = query['query_id']\n",
    "    query_text = query['query_text']\n",
    "    bm25_results[qid] = bm25_search(query_text, top_k=5)\n",
    "\n",
    "# Non-BM25 평가\n",
    "non_bm25_results = {}\n",
    "for query in queries:\n",
    "    qid = query['query_id']\n",
    "    query_text = query['query_text']\n",
    "    non_bm25_results[qid] = handle_accounting_non_bm25(query_text)\n",
    "\n",
    "bm25_evaluation = evaluate_all(bm25_results, queries, k=5)\n",
    "non_bm25_evaluation = evaluate_all(non_bm25_results, queries, k=5)\n",
    "\n",
    "print(\"BM25 Evaluation:\", bm25_evaluation)\n",
    "print(\"Non-BM25 Evaluation:\", non_bm25_evaluation)"
   ],
   "id": "a5eeb7b832a1d11f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 Evaluation: {'P@k': np.float64(0.05), 'R@k': np.float64(0.125), 'MRR': np.float64(0.0625), 'MAP': np.float64(0.0625)}\n",
      "Non-BM25 Evaluation: {'P@k': np.float64(0.15000000000000002), 'R@k': np.float64(0.375), 'MRR': np.float64(0.4583333333333333), 'MAP': np.float64(0.4583333333333333)}\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T09:22:09.192742Z",
     "start_time": "2025-07-16T09:22:09.181708Z"
    }
   },
   "cell_type": "code",
   "source": "pd.DataFrame([bm25_evaluation, non_bm25_evaluation], index=['BM25', 'Non-BM25'])",
   "id": "90c38197242b3c9b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           P@k    R@k       MRR       MAP\n",
       "BM25      0.05  0.125  0.062500  0.062500\n",
       "Non-BM25  0.15  0.375  0.458333  0.458333"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P@k</th>\n",
       "      <th>R@k</th>\n",
       "      <th>MRR</th>\n",
       "      <th>MAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BM25</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non-BM25</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 72
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
